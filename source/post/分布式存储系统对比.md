```toml
title = "分布式存储系统对比"
date = "2017-11-14 22:56:55"
update_date = "2017-11-14 22:56:55"
author = "KDF5000"
thumb = ""
tags = ["分布式系统，存储系统，对象存储"]
draft = false
```
### ceph
Ceph是一个统一的分布式存储系统，提供了对象存储，块存储和文件存储三种模式，在架构上没有中心节点，提供了无限的扩展性。

> Ceph is a unified, distributed storage system designed for excellent performance, reliability and scalability

Ceph核心是使用CRUSH算法通过计算获取数据存储的位置，无需中心节点，提供了高度的可靠性和可扩展性。
![](http://docs.ceph.com/docs/master/_images/stack.png)
ceph的的块存储，文件存储都是基于其实现的对象存储系统RADOS。下面是RADOS的系统架构
![1510306287044.jpg](@media/archive/blog/images/876E9FA342C9C0341021EDC0E7611B22.jpg?imageView2/0/w/600)

<!--more-->

OSD是存储对象的设备，Monotors提供整个OSDs布局的监控，也就是记录clusterMap, 然后client使用crush算法对输入的对象id,和monotors提供的clustermap映射到osd组，然后直接对对象进行存取。整个映射过程如下图:
![1510306450158.jpg](@media/archive/blog/images/DEA1B39C1679E4428E577CD8555A0AD4.jpg?imageView2/0/w/600)

参考论文：

1. [Ceph: A Scalable, High-Performance Distributed File System](https://www.ssrc.ucsc.edu/Papers/weil-osdi06.pdf)

2. [RADOS: A Scalable, Reliable Storage Service for Petabyte-scale Storage Clusters](https://ceph.com/wp-content/uploads/2016/08/weil-rados-pdsw07.pdf)

3.  [CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data](https://ceph.com/wp-content/uploads/2016/08/weil-crush-sc06.pdf)

### swift
swift是openstack项目的一个开源的分布式对象存储系统，使用一致性哈希算法确保真个系统没有中心节点，并且使用Quorum协议保证系统的最终一致性。下面是整个系统的架构：
![image005.png](@media/archive/blog/images/AC2D020F9C2A8DF9C6866217D265E57D.png?imageView2/0/w/600)
最核心的就是如何对一个对象进行寻址，swift抽象除了虚拟节点的概念，先讲对象使用一致性哈希算法映射到虚拟节点，然后利用虚拟节点到物理设备的映射表找到实际的存储设备。
![image003.png](@media/archive/blog/images/B222FF85DEDC4427DE45D2390B9BA1A8.png?imageView2/0/w/600)
上图中的分区P0-p7就是一致性哈希算法中的虚拟节点
![image001.png](@media/archive/blog/images/C5C82FF2AC2BD1C6E5087E44066B7E75.png?imageView2/0/w/600)
以逆时针方向递增的散列空间有 4 个字节长共 32 位，整数范围是[0~232-1]；将散列结果右移 m 位，可产生 232-m个虚拟节点，例如 m=29 时可产生 8 个虚拟节点。在实际部署的时候需要经过仔细计算得到合适的虚拟节点数，以达到存储空间和工作负载之间的平衡
### GlusterFS
GlusterFS(GNUF ClusterFile System)是一个开源的分布式文件系统，使用分布式哈希(DHT)技术去中心化存储，采用模块化的扩展架构，支持IP/RDMA等传输协议，提供Block, File, Object统一存储，下面是整体架构：
![WechatIMG248.jpeg](@media/archive/blog/images/D8673B563AC1F588B76B2AAD28F3999E.jpg?imageView2/0/w/600)
参考: [GlusterFS分布式文件系统](http://www.chinacloud.cn/upload/2015-01/15011009251270.pdf)
### fastdfs/tfs
fastdfs是一个国人开源的分布式文件系统，解决了大容量存储和负载均衡的问题，结构如下:
![](http://static.oschina.net/uploads/img/201204/20230218_pNXn.jpg)
tracker负责系统的调度工作，访问上起到负载均衡的作用，storage节点会定期发送心跳给每个tracker节点，告诉其所属group等信息，每个tracker是对等的，因此可以无限扩展，但是因为每个storage节点要发送心跳消息给所有tracker，所以也不能扩展很多。

每个storage节点在初始化是，会生成一个二级目录，每一级含有256个目录，也就是一共65525个文件目，在存储文件时使用round robin(轮询)、load balance(选择最大剩余空 间的组上传文件)、specify group(指定group上传)中的一种方式存储文件。每一个group的数据间异步进行复制(tfs是同步进行复制)。对于小文件的存储没有做特殊的优化处理。
参考: [FastDFS 分布式存储实战](@media/archive/blog/images/FastDFS.pdf)
### HDFS
HDFS是专门为GB级别的数据存储优化的存储系统，namenode负责元数据的存储，datanode负责实际文件的存储，大文件分成64m的chunck进行存储，使用chain replication的方式对数据进行复制。
![](@media/archive/hdfs_arch.png)

参考：[http://blog.csdn.net/suifeng3051/article/details/48548341](http://blog.csdn.net/suifeng3051/article/details/48548341)
### Haystack
- WebServer + Haystack Directory + CDN + Cache(Internal CDN) + Haystack Store
- 多级缓存，映射关系集中管理
- 小文件合并，内存索引，多副本
![haystack_arch.jpeg](@media/archive/blog/images/22537BD9E2E878215589256DE92F4DF7.jpg?imageView2/0/w/600)
![图片 1.png](@media/archive/blog/images/8BCBA869054800DD24B9D624136E0051.png?imageView2/0/w/600)
### Ambry
- 系统架构和haystack很像，核心思想也是小文件合并
- 各种大小文件，大文件切分
- 数据负载均衡
- 文件索引分段，冷数据持久化，布隆过滤快速定位
- 跨多个数据中心
![图片 1.png](@media/archive/blog/images/64943ED7F96443B5505E254ABC345E7B.png?imageView2/0/w/600)
